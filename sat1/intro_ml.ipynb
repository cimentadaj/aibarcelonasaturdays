{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "# rows and columns are usually reffered to as M x N\n",
    "# coefficients are usualy called parameters or weights in ML\n",
    "# It makes, sens, a coefficient nn is multiplied by a vector x = (1, 2, 3), so it's like a weight.\n",
    "\n",
    "# Cost function is a function that calculates whether the predictions of a set of parameters\n",
    "# is a good prediction. So it basically evaluates the predictions of a set of paramteres.\n",
    "# In linear models it is the root sum squared error or RSSE\n",
    "\n",
    "# In principle there could be other cost functions. You could create one yourself to evaluate\n",
    "# lower or higher than values rather than the overall values.\n",
    "\n",
    "# The cost function is highly dependent on the same sample size. Makes sense. If a new\n",
    "# row is added with extreme values, then the cost function is highly variable.\n",
    "\n",
    "# It is for this reason that we get the R-MEAN-SE. So we get the mean/median of the squared errors.\n",
    "\n",
    "# The evaluation of the model is tried on every single value of every parameter\n",
    "# and with every combination of numbers it evaluates the cost function. It gets\n",
    "# to a point where it finds the minimum value where on both sides in\n",
    "# would be higher. This is called the gradient descent\n",
    "\n",
    "# -       -\n",
    "#  -     -\n",
    "#   -   -\n",
    "#    - -\n",
    "#     X # the local minimum!!\n",
    "# --------------------\n",
    "#\n",
    "\n",
    "\n",
    "# How do you control the tolerance or change in the gradient descent? With\n",
    "# alpha or hyper-parameter. A little alpha means that you need more iterations\n",
    "# to find a local minimum. A higher alpha can on the other hand increase\n",
    "# the cost function.\n",
    "\n",
    "# There is different values to alpha but also different stragigies. You can begin\n",
    "# with a big alpha/hyper paramter and when you're within a certain slice of the\n",
    "# curve, change to a smaller alpha.\n",
    "\n",
    "# In other scenarions you can calculate the derivative of the function right away\n",
    "# without calculating the minimum local. (This I don't understand very much.)\n",
    "# They said that if you know the shape of the linear combination you can\n",
    "# calculate the derivative without applying a gradient descent.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
